\chapter{Forecasting Equipment Sensors}
\label{chap:methodology}
\setlength{\parskip}{1em}

\section{Aim}

The primary aim of this research aligns with the strategic interests of Eurasian Resources Group (ERG) in modernizing and optimizing their manufacturing facilities across Kazakhstan. Our shared vision focuses on leveraging advanced technologies to enhance industrial operations through data-driven decision-making and predictive maintenance capabilities. This collaboration emerged from mutual recognition of the potential to significantly improve operational efficiency and reduce unplanned downtime in industrial facilities.

The fundamental objective is to develop a sophisticated model that employs machine learning algorithms to process and analyze sensor data from industrial equipment. This model aims to identify anomalous behavior patterns and predict potential equipment failures before they occur, thereby enabling proactive maintenance interventions. By focusing on early detection of equipment deterioration and potential failures, the system seeks to minimize production disruptions and optimize maintenance resource allocation.

Furthermore, we aim to create an open-source solution that can be readily adopted by various industrial facilities, not limited to ERG's operations. This approach reflects our commitment to contributing to the broader development of Kazakhstan's industrial sector. The solution is designed to be highly configurable, allowing adaptation to different types of industrial equipment and varying sensor configurations. This flexibility ensures that the system can be implemented across diverse industrial environments and equipment types.

A key consideration in our aims is the development of a solution that is easy to deploy and maintain. This includes creating comprehensive documentation, implementing user-friendly interfaces, and ensuring compatibility with existing industrial infrastructure. The system is designed to integrate seamlessly with common industrial data collection platforms while requiring minimal specialized expertise for deployment and operation.

Through these objectives, we seek to bridge the gap between advanced analytical capabilities and practical industrial applications, providing a valuable tool for enhancing the operational efficiency and reliability of manufacturing facilities in Kazakhstan and potentially beyond. The successful achievement of these aims would represent a significant step forward in the modernization of industrial maintenance practices and the adoption of Industry 4.0 principles in the region.

\section{Objectives}

Building upon the established aims and considering the specific context of ERG's manufacturing facilities in Kazakhstan, this research pursues the following detailed objectives:

Conduct a comprehensive analysis of current equipment monitoring practices and maintenance strategies within ERG's ferrous alloy smelting facilities, with particular focus on the thermal-ore furnaces in Aksu. This analysis includes identifying inefficiencies in existing maintenance approaches, evaluating the limitations of current monitoring systems, and assessing the economic impact of unplanned equipment downtime. The assessment will provide crucial insights into areas where predictive maintenance can deliver the most significant improvements.

Establish an efficient data pipeline for collecting and preprocessing sensor data from industrial equipment. This involves developing robust methods for handling the continuous stream of data from AVEVA Historian, implementing appropriate data cleaning procedures, and creating standardized formats for data storage and retrieval using SQL Server. Special attention will be given to maintaining data integrity while dealing with various sensor types and sampling rates from the thermal-ore furnaces' electrode monitoring systems.

Investigate and evaluate various machine learning algorithms suitable for industrial time-series analysis and anomaly detection. This objective includes conducting comparative analyses of different approaches, considering factors such as prediction accuracy, computational efficiency, and real-time processing capabilities. The selection process will prioritize algorithms that can effectively handle the specific characteristics of industrial sensor data while maintaining interpretability of results.

Design and implement a machine learning model specifically tailored for predictive maintenance in metallurgical operations. The model development will focus on early detection of anomalies in electrode performance and prediction of potential failures in thermal-ore furnaces. This includes creating appropriate feature engineering methods, developing model training procedures, and implementing validation protocols to ensure reliable performance.

Validate the developed model's effectiveness using historical operational data from ERG's facilities. This involves conducting rigorous testing using real-world sensor data, evaluating the model's predictive accuracy, and assessing its practical utility in identifying maintenance requirements. The validation process will include measuring key performance indicators such as prediction accuracy, false alarm rates, and advance warning time before potential failures.

Create an open-source framework that encapsulates the entire solution, from data acquisition to prediction generation. This framework will be designed with modularity and configurability in mind, allowing easy adaptation to different industrial environments and equipment types. The development will include creating comprehensive documentation, implementing user-friendly interfaces, and ensuring compatibility with common industrial data systems.

Establish deployment protocols and guidelines for implementing the solution in industrial environments. This includes developing installation procedures, creating maintenance documentation, and providing training materials for operational staff. The protocols will be designed to minimize disruption to existing operations while ensuring effective integration with current industrial systems.

These objectives are structured to systematically address the challenges identified in the research work section while fulfilling the aims of both the academic research and ERG's practical needs. The successful completion of these objectives will result in a practical, deployable solution that can significantly enhance the maintenance practices in Kazakhstan's manufacturing facilities while contributing to the broader field of industrial predictive maintenance.

\section{Significance of Study}
The significance of this research extends across multiple dimensions, from immediate practical applications in industrial settings to broader implications for Kazakhstan's manufacturing sector. This study addresses critical challenges in industrial maintenance while offering substantial benefits for operational efficiency and resource optimization.

Primarily, this research facilitates a fundamental shift in maintenance paradigms, enabling manufacturing facilities to transition from traditional reactive maintenance approaches to data-driven predictive maintenance strategies. This transformation is particularly significant for facilities like ERG's operations in Kazakhstan, where unplanned equipment failures can result in substantial production losses and costly repairs. By developing a system capable of predicting potential failures before they occur, facilities can move away from the inefficient "fix-when-broken" approach to a more sophisticated, proactive maintenance strategy.

The economic significance of this research is substantial, as it directly addresses key operational challenges in industrial settings. By enabling early detection of equipment anomalies and potential failures, the proposed solution can significantly reduce unplanned downtime, which is often one of the largest sources of productivity loss in manufacturing operations. Furthermore, the ability to predict maintenance requirements allows for more efficient resource allocation, enabling maintenance teams to plan interventions during scheduled downtimes and optimize spare parts inventory management. This proactive approach not only reduces immediate maintenance costs but also contributes to extending equipment lifetime through timely interventions, representing significant long-term cost savings for industrial operators.

A crucial aspect of this study's significance lies in its focus on creating an accessible and efficient solution. The development of an open-source, configurable framework addresses a common barrier to adoption of advanced maintenance systems: the complexity and cost of implementation. By reducing setup time and minimizing the specialized labor required for deployment, the solution makes advanced predictive maintenance capabilities accessible to a broader range of industrial facilities. This accessibility is particularly important in the context of Kazakhstan's industrial sector, where there is a growing need for modernization and efficiency improvements.

The research also carries significant implications for industrial safety and environmental protection. By helping to prevent equipment failures and maintaining optimal operating conditions, the predictive maintenance system can reduce the risk of accidents and minimize environmental impacts associated with equipment malfunctions. This aspect is particularly relevant for metallurgical operations, where equipment failures can have serious safety and environmental consequences.

From an academic perspective, this research contributes to the growing body of knowledge in industrial artificial intelligence and predictive maintenance. The development of specific algorithms and methodologies for processing industrial sensor data, particularly in the context of metallurgical operations, adds valuable insights to the field. The open-source nature of the solution ensures that these contributions can be built upon by other researchers and practitioners, fostering further innovation in industrial maintenance technologies.

Moreover, the study's significance extends to workforce development and industrial modernization. By implementing advanced predictive maintenance systems, facilities not only improve their operational efficiency but also create opportunities for workforce upskilling and technological advancement. This aligns with broader industrial development goals and contributes to the evolution of Kazakhstan's manufacturing sector towards Industry 4.0 standards.

The practical implementation of this research at ERG's facilities serves as a valuable case study for similar industrial operations, demonstrating the feasibility and benefits of advanced predictive maintenance systems in real-world settings. This can encourage wider adoption of similar technologies across Kazakhstan's industrial sector, contributing to overall industrial modernization and competitiveness.

\section{Hardware and Operating System}

Our methodology began with assembling a computing platform capable of both fast data handling and accelerated model training. We relied on a laptop equipped with an NVIDIA RTX 4060 GPU (CUDA compute capability 8.9) alongside an Intel i5-13450HX CPU. Arch Linux was chosen as the operating system for its minimal footprint and configurability, and we installed the official NVIDIA drivers to enable full GPU acceleration via CUDA. This configuration allowed us to run parallelized training workloads while keeping data preprocessing responsive.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/01_fastfetch.png}
    \caption{Output of the fastfetch cli tool for general system information}
\end{figure}

\section{Development Environment}

Building on this hardware base, we established a reproducible software environment centered on Visual Studio Code. Inside VS Code we created a Python virtual environment to isolate project dependencies, then used UV to install the libraries needed for data ingestion, feature engineering, model training and evaluation. To track our progress and coordinate changes, every script, configuration file and experiment log was committed to a Git repository. This setup ensured that each iteration remained transparent, reversible and easy for team members to share.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/02_github.png}
    \caption{Directory structure of the project seen on GitHub}
\end{figure}

\section{Data Pre-processing}
We began by inspecting the raw sensor logs to understand the scope of missing entries. We noticed several gaps where readings were simply absent, so we filled each empty slot with the closest available value from adjacent timestamps. For gaps at the very start or end of a series, we applied forward or backward filling to maintain continuity. While cleaning, we also discovered entire sensor channels that had never recorded any data in real production. Those features never contributed meaningful information, so we removed them from our dataset. This pruning step reduced noise and focused our models on the signals that actually matter.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/03_csv.png}
    \caption{CSV file containing dataset}
\end{figure}

To capture cyclical patterns in equipment behavior, we transformed time-based features into continuous circular representations. Hours and weekdays, being cyclical in nature, can't be used directly as numeric values since this would create artificial discontinuities (e.g., hour 23 to 0, or Sunday to Monday). Instead, we encoded these temporal features using sine and cosine transformations.
For hours, we applied the following transformations:

\(\text{hour\_sin} = \sin\left(\frac{2 \pi \cdot \text{hour}}{24}\right)\)

\(\text{hour\_cos} = \cos\left(\frac{2 \pi \cdot \text{hour}}{24}\right)\)

Similarly, for weekdays (0-6 representing Monday through Sunday):

\(\text{weekday\_sin} = \sin\left(\frac{2 \pi \cdot \text{weekday}}{7}\right)\)

\(\text{weekday\_cos} = \cos\left(\frac{2 \pi \cdot \text{weekday}}{7}\right)\)

This encoding ensures that similar times of day or adjacent days have similar numerical representations. For example, 23:00 and 00:00 become close in the transformed space, reflecting their temporal proximity. 

These transformed features helped our LSTM model identify patterns tied to specific times of day (like shift changes or daily maintenance routines) and weekly cycles (such as weekend maintenance periods or production schedules). The continuous nature of these features significantly improved the model's ability to learn time-dependent patterns in equipment behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/12_hour_sine.png}
    \caption{Sine graph showing period of 24 hours}
    \label{fig:sine-graph}
\end{figure}

\section{Machine Learning Model Comparison}
We evaluated LSTM, CNN and Transformer architectures to see which best suited our needs. Prototypes of each were trained on a representative slice of machine data so we could compare training speed, memory use and predictive accuracy. CNNs extracted local patterns effectively but required more GPU memory and compute time than our setup could sustain for frequent retraining. Transformers captured long-range dependencies well but proved too heavy to train from scratch on each update cycle. LSTMs struck the right balance: they learned temporal relationships without overloading our laptop’s GPU or CPU. We were able to refresh LSTM models continuously as new data arrived, so we chose them as the backbone of our downtime‐prediction pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/04_model_comparison.png}
    \caption{Comparison between different models}
\end{figure}

\section{Creating LSTM model}
We designed our neural network as a sequential model in Keras, carefully structuring each layer to process time-series equipment data. The input layer accepts sequences of 24 consecutive readings, which represents a full day of sensor measurements chunked into hourly blocks. This sequence length lets the model learn daily patterns while keeping memory requirements manageable. Following the input, we added an LSTM layer with 40 units and ReLU activation, allowing it to capture temporal dependencies in the data without running into vanishing gradient issues. The LSTM's output feeds into a Dense layer of 20 units, also using ReLU activation, which helps the network learn higher-level feature combinations. The final Dense layer narrows down to a single unit, producing one predicted value that represents the likelihood of equipment failure. 

We chose the Adam optimizer for its ability to handle noisy gradients and automatically adjust learning rates. Mean squared error serves as our loss function since we're essentially dealing with a regression problem – predicting a continuous value that represents failure probability. This architecture strikes a balance between model capacity and training efficiency, letting us retrain quickly when new data arrives while maintaining enough complexity to catch subtle patterns in machine behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/05_lstm.png}
    \caption{Layers of sequential model with LSTM and Dense layers}
\end{figure}

During model development, we compared three common activation functions: ReLU (Rectified Linear Unit), tanh (hyperbolic tangent), and sigmoid. Each function processes neuron inputs differently, affecting both model performance and computational efficiency.
The ReLU function, defined as \(f(x) = max(0, x)\), simply zeroes out negative values while passing positive values unchanged. The tanh function, \(f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}\), squashes inputs to the range [-1, 1], while sigmoid, \(f(x) = \frac{1}{1 + e^{-x}}\), compresses values to [0, 1]. 

While tanh and sigmoid provide smooth, bounded outputs, they require exponential calculations that increase computational overhead. Both also suffer from vanishing gradient problems when networks become deep, as their derivatives are bounded: \(f'(x) \leq \frac{1}{4}\) for sigmoid and \(f'(x) \leq 1\) for tanh.

ReLU, despite its simplicity, showed comparable prediction accuracy while requiring significantly fewer computational resources. Its derivative is either 0 or 1, making gradient calculations trivial. This efficiency became crucial for our real-time prediction requirements, where new data constantly streams in. While tanh and sigmoid might theoretically capture more complex patterns, the practical benefits of ReLU's speed and stability led us to implement it in both LSTM and Dense layers. This choice helped us achieve our target inference speed without sacrificing meaningful prediction accuracy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{chapters/02_methodology/figures/13_relu.png}
    \caption{Graph of ReLU activation function}
    \label{fig:relu-graph}
\end{figure}

\section{Model Export and Inference}
We packaged our trained LSTM model into a production-ready format, saving the network architecture and weights in Keras's native format. Alongside the model, we preserved the data scalers using joblib to ensure new inputs get normalized exactly like our training data. These exports formed the core of a FastAPI application we built for real-time predictions. The API accepts date ranges and returns minute-by-minute failure probability estimates, matching the granularity of our source dataset. We designed the endpoint to be flexible - it can load different model versions and handle varying time windows without needing to restart the service. When a request comes in, the API loads the appropriate model and scalers, processes the input timestamps, and streams back predictions that maintenance teams can act on. This setup lets us swap in improved models as they're developed while keeping the prediction interface consistent for end users.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/03_results/figures/02_test_mse_error.png}
    \caption{Comparison chart of test mean squarred error (MSE) values accross models}
    \label{fig:test-mse-error}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{chapters/02_methodology/figures/06_export.png}
    \caption{Exported Keras models and joblib scalers}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{chapters/02_methodology/figures/14_exported.png}
    \caption{}
    \label{fig:Diagram representing artifacts of model training}
\end{figure}

\section{Digital Twin}
We built a virtual replica of the manufacturing equipment to help validate our predictions against real-world behavior. Since we couldn't tap into live sensor feeds, we created a system that cycles through our historical dataset to simulate ongoing equipment operation. The digital twin runs as a FastAPI service, offering endpoints that mirror how real sensors would report their readings. When queried, it returns minute-by-minute values for any sensor between specified start and end times, just like the actual equipment would. This setup lets us run side-by-side comparisons between our model's predictions and the "real" values from our simulated machinery. Having this twin running alongside our prediction service proved invaluable for testing - we could verify model accuracy, experiment with different prediction windows, and spot any drift between expected and actual readings. The twin also helps demonstrate our system to stakeholders, showing how predictions line up with equipment behavior without needing access to the production floor.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{chapters/02_methodology/figures/07_twin.png}
    \caption{JSON response from Digital Twin with values from equipment}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{chapters/02_methodology/figures/11_digital-twin-digram.png}
    \caption{Diagram showcasing workflow of Digital Twin}
    \label{fig:digital-twin-workflow}
\end{figure}

\section{Data Gateway and Storage}
We developed a gateway service to bring together readings from our digital twin and predictions from our LSTM model. This service acts as a central hub, fetching and aligning data from both sources to give us a complete picture of predicted versus actual equipment behavior. To handle the growing volume of time-series data efficiently, we integrated TimescaleDB as our storage layer. The database automatically organizes readings by time chunks, making queries for specific date ranges lightning fast. When users request comparisons between real and predicted values, the gateway first checks if we already have those calculations stored. If found, it serves them directly from TimescaleDB instead of regenerating predictions and fetching twin data again. This caching strategy cut response times dramatically, especially for commonly accessed time periods. The gateway also handles data cleanup, pruning old records we don't need while keeping recent history readily available for analysis. This combination of smart caching and time-series optimization lets us serve comparative analyses quickly, even as our dataset grows.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/08_gateway.png}
    \caption{Architecture of the gateway service with TimescaleDB}
\end{figure}

\section{Frontend}
We built a web interface using React and Next.js to make our prediction system accessible and easy to visualize. The frontend lets users explore equipment behavior through an intuitive dashboard layout. We chose TailwindCSS for styling, which helped us create a clean, responsive design without writing custom CSS. ReCharts handles all our data visualization needs, displaying both predicted and actual sensor values on interactive time-series graphs. Users can select specific sensors from a dropdown menu and set custom date ranges using two date pickers. When new dates or sensors are selected, the interface fetches data through our gateway and updates the charts in real-time. The graphs automatically adjust their scale and detail level based on the selected time window, making it easy to spot patterns or anomalies. This setup gives maintenance teams a straightforward way to monitor equipment health and validate our prediction accuracy across different timeframes.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{chapters/02_methodology/figures/09_frontend.png}
    \caption{A view of predicted and real data on the frontend}
    \label{fig:frontend-filters}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{chapters/02_methodology/figures/10_frontend_logic.png}
    \caption{Diagram describing workflow on frontend side}
    \label{fig:frontend-logic}
\end{figure}